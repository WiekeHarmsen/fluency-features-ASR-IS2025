{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from utils import read_textgrids as rf\n",
    "import sclite.sclite_string_normalizer as sclite_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTextGridFile(tgFile, corpus):\n",
    "    # Read TextGrid file\n",
    "    if corpus == 'serda':\n",
    "        tg_df = rf.read_tg_file_to_df(tgFile, 'latin-1')\n",
    "    elif corpus == 'jasmin':\n",
    "        tg_df = rf.read_tg_file_to_df_jasmin(tgFile)\n",
    "    return tg_df.astype({'start_time':float, 'end_time':float} )\n",
    "\n",
    "def selectWordTierTextGrid(tg_df, word_tier_name):\n",
    "    return tg_df[tg_df['tier_name'] == word_tier_name]\n",
    "\n",
    "def splitTextDFIntoSentences(tg_df_orth_trans):\n",
    "    sentenceDFList = []\n",
    "    startIDX = tg_df_orth_trans.index[0]\n",
    "    for idx, row in tg_df_orth_trans.iterrows():\n",
    "        if row['text'][-1] in ['.', '!', '?']:\n",
    "            sentenceDFList.append(tg_df_orth_trans.loc[startIDX:idx, :])\n",
    "            startIDX = idx+1\n",
    "    return sentenceDFList\n",
    "\n",
    "def wordRowToWordSegment(row):\n",
    "    # Remove annotation tags (*u, *a, etc.), remove all punctuation except the basic punctuation (!-'.?) and all default normalization steps (poss. pronouns, names, spelling errors, write numbers as words)\n",
    "    w = sclite_norm.normalize_string(row['text'], annTags=True, all_punct=False, basic_punct=True)\n",
    "    if ' ' in w:\n",
    "        print(w)\n",
    "\n",
    "    return {\n",
    "                \"text\": w.replace(' ', ''),\n",
    "                \"start\": row['start_time'],\n",
    "                \"end\": row['end_time'],\n",
    "                \"confidence\": 0.0\n",
    "            }\n",
    "\n",
    "\n",
    "def turnSentenceDFIntoSegment(sentenceDF, sentenceNr):\n",
    "\n",
    "    # Remove _ words, these are noisy areas before/after words\n",
    "    sentenceDF = sentenceDF[sentenceDF['text'] != '_']\n",
    "\n",
    "    wordsList = list(sentenceDF.apply(wordRowToWordSegment, axis=1))\n",
    "\n",
    "    return {\n",
    "            \"id\": sentenceNr,\n",
    "            \"seek\": 0,\n",
    "            \"start\": sentenceDF.loc[sentenceDF.index[0], 'start_time'],\n",
    "            \"end\": sentenceDF.loc[sentenceDF.index[-1], 'end_time'],\n",
    "            \"text\": \" \".join([x['text'] for x in wordsList]),\n",
    "            \"words\": wordsList\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare JASMIN-NL and JASMIN-VL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z egt\n",
      "b il\n",
      "j jan\n",
      "laa t\n",
      "klim op school\n",
      "z oek t\n",
      "klim scho\n",
      "bus hak\n",
      "klim op\n",
      "kind je\n",
      "mama s\n",
      "nee ee\n",
      "s sjort\n",
      "nee ee\n",
      "schoola arts\n",
      "b bril\n",
      "re reporter\n",
      "gei nig\n",
      "uit kiezen\n",
      "kamer ploeg\n",
      "uits lover\n",
      "kampeer aploeg\n",
      "pret model\n",
      "aa rzelt\n",
      "daarn om\n",
      "etens tijd\n",
      "artis j jok harten\n",
      "artis jokharten\n",
      "dieren harten\n",
      "s pelt\n",
      "op pas adres\n",
      "automo mobilisten\n",
      "ar ti sjok harten\n",
      "slangenha harten\n",
      "schapendoe does\n",
      "handgebran gebarend\n",
      "ar ti sjok harten\n",
      "s pelt\n",
      "el kaar\n",
      "la ter\n",
      "artisjokha rten\n",
      "har ten\n",
      "toren hoog\n",
      "huis houden\n",
      "artisjok harten\n",
      "roof tocht\n",
      "toe fje\n",
      "me ter\n",
      "af gesproken\n",
      "postduivenve vereniging\n",
      "84 .json files created in /vol/tensusers2/wharmsen/JASMIN-fluency-features/comp-q-read_vl_age7-11_nat/06_manual_fluency_features/json-orth-trans\n",
      "/vol/tensusers2/wharmsen/JASMIN-fluency-features/comp-q-read_vl_age7-11_nat/06_manual_fluency_features/ot-norm.csv file created. This file is fully normalized, in contrast to the generated json files, also the basic punctuation is removed (!-'.?)\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "### Define inputs ###\n",
    "#####################\n",
    "\n",
    "# Read recordingsDF\n",
    "# basePath = '/vol/tensusers2/wharmsen/JASMIN-fluency-features/comp-q-read_nl_age7-11_nat'\n",
    "basePath = '/vol/tensusers2/wharmsen/JASMIN-fluency-features/comp-q-read_vl_age7-11_nat'\n",
    "recDF_path = os.path.join(basePath, '03_metadata/recordingsDF.tsv')\n",
    "\n",
    "# Set corresponding TextGrid dir with orthographic transcriptions\n",
    "tgDir = os.path.join(basePath, '00_orig_data/textgrids')\n",
    "tgExtension = '_updated.TextGrid'\n",
    "\n",
    "# Set corpus\n",
    "corpus = 'jasmin'\n",
    "\n",
    "# Create output dir\n",
    "outputDir = os.path.join(basePath, '06_manual_fluency_features/json-orth-trans')\n",
    "\n",
    "# Define output transcription files\n",
    "otTrans_norm = []\n",
    "outputTranscriptsNormFile = os.path.join(basePath, '06_manual_fluency_features/ot-norm.csv')\n",
    "\n",
    "\n",
    "#########################\n",
    "### Create JSON files ###\n",
    "#########################\n",
    "\n",
    "if not os.path.exists(outputDir):\n",
    "    os.makedirs(outputDir)\n",
    "\n",
    "# Read recordingsDF\n",
    "recDF = pd.read_csv(recDF_path, sep= '\\t', index_col=0)\n",
    "\n",
    "for audioID, row in recDF.iterrows():\n",
    "\n",
    "    # Extract appropriate metadata of each recording\n",
    "    startTimeRec = row['startTimeFirstSent']\n",
    "    endTimeRec = row['endTimeLastSent']\n",
    "    cutStart = row['cutStart']\n",
    "    cutEnd = row['cutEnd']\n",
    "    totalDuration = row['duration']\n",
    "\n",
    "    # Read TextGrid File\n",
    "    aviLevel = audioID.split('-')[1].split('_')[0].replace('AVI', 'AVI ')\n",
    "    recordingID = row['recordingID']\n",
    "    tgFile = os.path.join(tgDir, aviLevel + '/' + recordingID + tgExtension)\n",
    "    tg_df = readTextGridFile(tgFile, corpus)\n",
    "    word_tier_name = tg_df.loc[0,'tier_name']\n",
    "    tg_df_orth_trans = selectWordTierTextGrid(tg_df, word_tier_name)\n",
    "\n",
    "    # Only JASMIN: select part of TextGrid that belongs to the specific story and correct the time stamps\n",
    "    tg_df_orth_trans = tg_df_orth_trans[tg_df_orth_trans['start_time']>=cutStart]\n",
    "    tg_df_orth_trans = tg_df_orth_trans[tg_df_orth_trans['end_time']<=cutEnd]\n",
    "    tg_df_orth_trans['start_time'] = tg_df_orth_trans['start_time'] - cutStart\n",
    "    tg_df_orth_trans['end_time'] = tg_df_orth_trans['end_time'] - cutStart\n",
    "\n",
    "    # Split textDF into sentences\n",
    "    sentenceDFList = splitTextDFIntoSentences(tg_df_orth_trans)\n",
    "\n",
    "    # Change each sentenceDF to a segment\n",
    "    segmentList = [turnSentenceDFIntoSegment(sentenceDF, sentenceNr) for sentenceNr, sentenceDF in enumerate(sentenceDFList)]\n",
    "\n",
    "    # Save output as Dict\n",
    "    tgDict = {\n",
    "        \"text\" : \" \".join([segment['text'] for segment in segmentList]),\n",
    "        \"segments\" : segmentList,\n",
    "    }\n",
    "\n",
    "    # OUTPUT 1: original transcriptions\n",
    "    otTrans_norm.append([audioID, sclite_norm.normalize_string(tgDict['text'])])\n",
    "\n",
    "    # OUTPUT 2: Write tgDict as json file\n",
    "    outputFile = os.path.join(outputDir, audioID + '.json')\n",
    "    # Check if file already exists, if it does, only dump file if current raterNr == 1 (Some files are rated by two raters, we only use the ratings by rater A01 (raterNr = 1))\n",
    "    with open(outputFile, \"w\") as outfile:\n",
    "        json.dump(tgDict, outfile, indent=4)\n",
    "\n",
    "print(str(len(glob.glob(os.path.join(outputDir, '*.json')))) + ' .json files created in '+ outputDir)\n",
    "\n",
    "# Write ot-norm file\n",
    "otTrans_norm_DF = pd.DataFrame(otTrans_norm, columns=['audioID', 'orthographic_transcription'])\n",
    "otTrans_norm_DF.to_csv(outputTranscriptsNormFile, index=False)\n",
    "print(outputTranscriptsNormFile, 'file created. This file is fully normalized, in contrast to the generated json files, also the basic punctuation is removed (!-\\'.?)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare SERDA-comp1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 TextGrid files found\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/QPPY5-story_2-20230120114816570_A01_full_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/QTB2S-story_3-20221216105258185_A01_full_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/SMVCS-story_1-20221103091343185_A01_full_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/VJCMQ-story_2-20221123111647247_A01_full_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/WHHXX-story_3-20221107134241743_A01_full_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/XSWMB-story_1-20221216115257809_A01_full_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/YHTKC-story_1-20221117135859005_A03_orth_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/YJ3PN-story_2-20230109112554434_A03_orth_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/YKGD3-story_3-20221213115329236_A03_orth_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/YKKTR-story_1-20221128103838446_A03_orth_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/YMRDV-story_2-20221107093630712_A03_orth_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/YVBRP-story_3-20230117092704671_A01_full_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/YVBRP-story_3-20230117092704671_A03_orth_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/YWPWY-story_2-20230120093546063_A01_full_punct.TextGrid\n",
      "an der wets\n",
      "h hoor\n",
      "be kijkt\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/YWPWY-story_2-20230120093546063_A03_orth.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/Z2BYD-story_1-20221212135752996_A01_full_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/Z2BYD-story_1-20221212135752996_A03_orth_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/ZDPNZ-story_1-20221107124621000_A01_full_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/ZMQG2-story_2-20221128114456538_A01_full_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/ZNNGY-story_3-20230110103621268_A01_full_punct.TextGrid\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/00_orig_data/textgrids/YQFGF-story_3-20230116132345386_A03_orth_punct.TextGrid\n",
      "18 .json files created in /vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/06_manual_fluency_features/json-orth-trans\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/06_manual_fluency_features/ot-norm.csv file created. This file is fully normalized, in contrast to the generated json files, also the basic punctuation is removed (!-'.?)\n",
      "/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1/06_manual_fluency_features/ot-all.csv file created. This file is fully normalized, in contrast to the generated json files, also the basic punctuation is removed (!-'.?)\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "### Define inputs ###\n",
    "#####################\n",
    "\n",
    "corpus = 'serda'\n",
    "basePath = '/vol/tensusers2/wharmsen/SERDA-fluency-features/comp1'\n",
    "tgFileDir = os.path.join(basePath, '00_orig_data/textgrids')\n",
    "tgFileExtension = '.TextGrid'\n",
    "outputDir = os.path.join(basePath, '06_manual_fluency_features/json-orth-trans')\n",
    "word_tier_name = 'attempts'\n",
    "\n",
    "#########################\n",
    "### Create JSON files ###\n",
    "#########################\n",
    "\n",
    "# Create output dir\n",
    "if not os.path.exists(outputDir):\n",
    "    os.makedirs(outputDir)\n",
    "\n",
    "# Define output transcription files\n",
    "otTrans_norm = []\n",
    "otTrans_allFiles = []\n",
    "outputTranscriptsNormFile = os.path.join(basePath, '06_manual_fluency_features/ot-norm.csv')\n",
    "outputTranscriptsAllFile = os.path.join(basePath, '06_manual_fluency_features/ot-all.csv')\n",
    "\n",
    "# List all .TextGrid files\n",
    "# textgridFiles = glob.glob(os.path.join(tgFileDir, 'ZNNGY-story_3-20230110103621268_A01_full_punct*' + tgFileExtension))\n",
    "textgridFiles = glob.glob(os.path.join(tgFileDir, '*' + tgFileExtension))\n",
    "print(len(textgridFiles), 'TextGrid files found')\n",
    "\n",
    "for tgFile in textgridFiles:\n",
    "    print(tgFile)\n",
    "\n",
    "    # Read TextGrid file\n",
    "    tg_df = readTextGridFile(tgFile, corpus)\n",
    "    tg_df_orth_trans = selectWordTierTextGrid(tg_df, word_tier_name)\n",
    "\n",
    "    # Split textDF into sentences\n",
    "    sentenceDFList = splitTextDFIntoSentences(tg_df_orth_trans)\n",
    "\n",
    "    # Change each sentenceDF to a segment\n",
    "    segmentList = [turnSentenceDFIntoSegment(sentenceDF, sentenceNr) for sentenceNr, sentenceDF in enumerate(sentenceDFList)]\n",
    "\n",
    "    # Save output as Dict\n",
    "    tgDict = {\n",
    "        \"text\" : \" \".join([segment['text'] for segment in segmentList]),\n",
    "        \"segments\" : segmentList,\n",
    "    }   \n",
    "\n",
    "    # OUTPUTS\n",
    "    basename = os.path.basename(tgFile).split('_A0')[0]\n",
    "    raterNr = os.path.basename(tgFile).split('_A0')[1][0]\n",
    "\n",
    "    # OUTPUT 1: Original transcriptions\n",
    "    otTrans_allFiles.append([os.path.basename(tgFile).replace('.TextGrid', ''), sclite_norm.normalize_string(tgDict['text'])])\n",
    "\n",
    "    recID = basename.split('-2')[0]\n",
    "    if recID not in ['Z2BYD-story_1', 'YWPWY-story_2', 'YVBRP-story_3'] or raterNr == '1':\n",
    "        otTrans_norm.append([basename, sclite_norm.normalize_string(tgDict['text'])])\n",
    "\n",
    "    # OUTPUT 2: Write tgDict as json file\n",
    "    outputFile = os.path.join(outputDir, basename + '.json')\n",
    "\n",
    "    # Check if file already exists, if it does, only dump file if current raterNr == 1 (Some files are rated by two raters, we only use the ratings by rater A01 (raterNr = 1))\n",
    "    if (not os.path.exists(outputFile)) or (os.path.exists(outputFile) and raterNr == '1'):\n",
    "        with open(outputFile, \"w\") as outfile:\n",
    "            json.dump(tgDict, outfile, indent=4)\n",
    "\n",
    "print(str(len(glob.glob(os.path.join(outputDir, '*.json')))) + ' .json files created in '+ outputDir)\n",
    "\n",
    "# Write ot-norm file\n",
    "otTrans_norm_DF = pd.DataFrame(otTrans_norm, columns=['audioID', 'orthographic_transcription']).set_index('audioID').sort_index()\n",
    "otTrans_norm_DF.to_csv(outputTranscriptsNormFile)\n",
    "print(outputTranscriptsNormFile, 'file created. This file is fully normalized, in contrast to the generated json files, also the basic punctuation is removed (!-\\'.?)')\n",
    "\n",
    "# Write ot-all file\n",
    "otTrans_all_DF = pd.DataFrame(otTrans_allFiles, columns=['audioID', 'orthographic_transcription']).set_index('audioID').sort_index()\n",
    "otTrans_all_DF.to_csv(outputTranscriptsAllFile)\n",
    "print(outputTranscriptsAllFile, 'file created. This file is fully normalized, in contrast to the generated json files, also the basic punctuation is removed (!-\\'.?)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virenv-wav2vec2",
   "language": "python",
   "name": "virenv-wav2vec2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
